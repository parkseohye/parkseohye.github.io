## **Chapter 3. Probability Space**

Higher category **:** 【Statistics】 [Statistics Overview](https://classroom.tistory.com/32)

Kim, S.K., Park, J.B. (2022); [originally written in Korean](https://nate9389.tistory.com/1623)

---

**1.** set theory  

**2.** conditional probability 

---

**a.** [Inclusion-Exclusion Principle](https://classroom.tistory.com/52) 

**b.** [Monty Hall Problem](https://classroom.tistory.com/53) 

---

　
 
## **1. set theory** 

#### ⑴ case(outcome) **:** can be defined arbitrarily. marked as ωi  

#### ⑵ sample space **:** set Ω that satisfies the following conditions

> ① Ω = { ω<sub>1</sub>, ω<sub>2</sub>, ···, ω<sub>n</sub> }

> ② ω<sub>i</sub> and ω<sub>j</sub> are disjoint

> ③ Ω includes all possible results

> ④ ((1, 2, 3), (3, 4, 5, 6)) (×)

> ⑤ ((1, 2), (3, 4, 5, 6)) (○)

#### ⑶ field **:** a set F of all subsets of Ω

> ① assumption**:** Ω = (1, 2, 3) 

> ② F = (∅, (1), (2), (3), (1, 2), (1, 3), (2, 3), (1, 2, 3))

#### ⑷ events **:** each element of F, i.e. **subset** of sample space 

> ① total event **:** Ω 

> ② null event **:** ∅

> ③ complementary event **:** A<sup>c</sup> = Ω - A 

> ④ union event **:** A ∪ B

> ⑤ intersection event **:** A ∩ B

> ⑥ mutually exclusive events **:** relationship between events A and B that satisfy A ∩ B = ∅

> ⑦ when number of cases is n, the number of events is 2n 

#### ⑸ probability

> ① intuitive definition **:** probability of a particular event = number of a particular event ÷ number of all cases

> ② axiomatic approach **:** proposed by Kolmogorov, the father of statistics

>> ○ probability is a kind of function**:** F → ℝ

>> ○ **axiom 1**. P(E) ≥ 0

>> ○ **axiom 2**. P(Ω) = 1

>> ○ **axiom 3.** addition rule of probability**:** about disjoint Ei and Ej  

>> ○ **theorem 1.** P(∅) = 0 

>> ○ P(U) = P(U) + P(∅) ⇔ P(∅) = 0

>> ○ note that even though E ≠ ∅, P(E) can be 0

>> ○ **theorem** **2.** P(Ec) = 1 - P(E) 

>> ○ 1 = P(E ∪ Ec) = P(E) + P(E<sup>c</sup>)

>> ○ **theorem** **3.** E ⊂ F → P(E) ≤ P(F) 

>> ○ F = E ∪ (E<sup>c</sup> ∩ F) → P(F) = P(E) + P(E<sup>c</sup> ∩ F) ≥ P(E)

>> ○ **theorem** **4.** P(E ∪ F) = P(E) + P(F) - P(E ∩ F)

>> ○ E ∪ F = (Ec ∩ F) ∪ (E ∩ F) ∪ (E ∩ F<sup>c</sup>)

>> ○ P(E ∪ F) = (P(F) - P(E ∩ F)) + P(E ∩ F) + (P(E) - P(E ∩ F)) 

> ③ Cromwell's rule**:** it states that we should not use probabilities of 1 or 0, except for logical statements 

#### ⑹ inclusion-exclusion principle 

> ① example**:** P(A ∪ B ∪ C) = P(A) + P(B) + P(C) - P(A ∩ B) - P(B ∩ C) - P(C ∩ A) + P(A ∩ B ∩ C)  

<br>
<br>

## **2\. conditional** **probability**  

#### ⑴ conditional probability

> ① P(A ㅣ B)**:** the probability that event A will occur when event B already occurs

> ② expression**:** P(A ㅣ B) \= P(A ∩ B) ÷ P(B)

> ③ P(A ㅣ B) ≠ P(B ㅣ A) 

> ④ multiplication rule of probability**:** P(A ∩ B) = P(A) × P(B | A) = P(B) × P(A | B)

#### ⑵ chain rule of conditional probability

> ① example**:** P(A ∩ B ∩ C) = P(A) × P(B | A) × P(C | A ∩ B)  

#### ⑶ independence of events**:** if the conditional probability is equal to the intrinsic probability

> ① expression **:** P(A ∩ B) = P(A) × P(B)

> ② application

>> ○ P(A ㅣ B) = P(A ∩ B) ÷ P(B) = P(A)P(A B) = P(A ∩ B) ÷ P(B) = P(A)

>> ○ P(B ㅣ A) = P(A ∩ B) ÷ P(A) = P(B)

>> ○ E and F are independent ⇔ E<sup>c</sup> and F are independent ⇔ E and F<sup>c</sup> are independent ⇔ E<sup>c</sup> and F<sup>c</sup> are independent

>> ○ if P(A) = 0, A and B are always independent

>> ○ P(A), P(B) > 0, and A and B are independent → A and B are not disjoint

>> ○ if A and B are disjoint, P(A) = 0, P(B) = 0, or A and B are not independent (contrapositive proposition)

> ③ intuitive meaning

>> ○ information about one variable does not tell you any information about the other

> ④ relationship with conditional probabilities

>> ○ Pr(X = x ㅣ Y = y) = Pr(X = x)

#### ⑷ mutual independence

> ① **condition 1.** 

> ② **condition 2.** includes **condition 1**  

#### ⑸ conditional independence

> ① definition

> ② conditional independence is not always independent

> ③ independence is not always conditional independence

#### ⑹ law of total probability

> ① partition

>> ○ **definition 1.** (exhaustive) Ω = B<sub>1</sub> ∪ B<sub>2</sub> ∪ ··· ∪ B<sub>n</sub>

>> ○ **definition 2.** (disjoint) Bi ∩ Bj = ∅

> ② conclusion

>> ○ tip**:** drawing a Ben diagram is easy to understand

> ③ **important****:** used to induce Bayes' theorem

#### ⑺ Bayes' theroem

> ① **a****ssumption 1.** S = B<sub>1</sub> ∪ B<sub>2</sub> ∪ ··· ∪ B<sub>n</sub>

> ② **assumption 2.** ∀ i, j, B<sub>i</sub> ∩ B<sub>j</sub> = ∅

> ③ conclusion

> ④ **note 1.** a totally unrelated P (A | B) may be used to calculate P (B | A)

> ⑤ **note 2.** results (posterior, a posterior) cand predict causes (prior, a pryori)

>> ○ A **:** events observed as a result 

>> ○ B **:** a causal event

>> ○ P(B<sub>i</sub>) **:** **prior**

>> ○ P(B<sub>i</sub> ㅣ A)**:** when A is observed, the probability that Bi is the cause (**posterior**) 

>> ○ P(A ㅣ B<sub>i</sub>) **:** when Bi is the cause, the probability of A occurring (**likelihood**)

> ⑥ Bayes Statistic

>> ○ infers probabilities without identifying the population

>> ○ claims that probability is nothing but human belief

#### ⑻ **example** **1.** diagnostic rate

> ① percentage of cancer patients

<center>
P(A) = 1%
</center>

> ② positive probability (hit rate) at diagnosis

<center>
P(B)
</center>

> ③ positive probability when diagnosing cancer patients

<center>
P(B ㅣ A) = 95%
</center>

> ④ probability of false positive when diagnosing non-cancer patients

<center>
 P(B ㅣ A<sup>c</sup>) = 5%
</center>

> ⑤ **question****:** probability of a positive person really getting cancer during diagnosis

<center>
P(A ㅣ B)
</center>

> ⑥ proportion of positive in diagnosis**:** the law of total probability is used

<center>
 P(B) = P(B ㅣ A) + P(B ㅣ A<sup>c</sup>) = 0.95 × 0.01 + 0.05 × 0.99
</center>

> ⑦ probability that any person is a cancer patient and is positive in diagnosis

<center>
P(A ∩ B) = P(B ㅣ A) × P(A) = 0.95 × 0.01
</center>

 > ⑧ answer

<center>
P(A | B) = P(A ∩ B) ÷ P(B) = **0.16**
</center>

> ⑨ **interpretation****:** since there are many people who are not cancer patients, the false positive rate is quite high

#### ⑼ **example 2.** Monty hall problem

>> ① situation

> ○ supercar stands behind one of the three doors

> ○ show participant choose one of the three doors 

> ○ the show host opens any of the doors that the show participant did not open

> ○ show participant can stick to or change their existing selections

>> ○ question **:** which choice is reasonable?

> ② premise

>> ○ show participant first select door 1

>> ○ show host opens door 3

> ③ definition

>> ○ P(i): probability of supercar behind door 1

>> ○ P(ii): probability of supercar behind door 2

>> ○ P(iii): probability of supercar behind door 3

>> ○ P(Ⅰ): probability of the show host choosing door 1

>> ○ P(Ⅱ): probability of the show host choosing door 2

>> ○ P(Ⅲ) **:** probability of the show host choosing door 3

>> ○ P(i), P(ii), and P(iii) are priors (causes)

>> ○ P(Ⅰ), P(Ⅱ), and P(Ⅲ) are posteriors (results)

> ④ conditional probability

>> ○ P(ⅰ ㅣ Ⅲ) **:** when the show host selects door 3, the probability that there is a supercar behind door 1

>> ○ P(Ⅲ ㅣ ⅰ) **:** if there's a supercar behind door 1, the probability that the show host will choose door 3

> ⑤ Bayes ' theorem

>> ○ P(ⅰ ㅣ Ⅲ) 

>> ○ P(ⅱ ㅣ Ⅲ)

>> ○ P(ⅲ ㅣ Ⅲ) 

> ⑥ conclusion **:** it's advantageous for show participant to change their choices
